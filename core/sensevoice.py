import warnings
warnings.filterwarnings("ignore")

import os
import sys
import torch
from typing import Dict
import librosa
from rich import print as rprint
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
import subprocess
import tempfile
import pandas as pd
import logging

# å¯¼å…¥ SenseVoice ç›¸å…³åº“
from model import SenseVoiceSmall
from funasr.utils.postprocess_utils import rich_transcription_postprocess

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# å¯¼å…¥å…¶ä»–è‡ªå®šä¹‰æ¨¡å—
from core.config_utils import load_key
from core.all_whisper_methods.demucs_vl import demucs_main
from core.all_whisper_methods.whisperXapi import (
    convert_video_to_audio, 
    split_audio, 
    save_results, 
    save_language
)
from core.all_whisper_methods.whisperXapi import (
    RAW_AUDIO_FILE, 
    BACKGROUND_AUDIO_FILE, 
    VOCAL_AUDIO_FILE, 
    AUDIO_DIR
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('transcription.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å°è¯•ä»é…ç½®è·å–æ¨¡å‹ç›®å½•ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
try:
    MODEL_DIR = load_key("model_dir")
except Exception as e:
    logger.warning(f"æ— æ³•åŠ è½½æ¨¡å‹ç›®å½•é…ç½®ï¼š{e}")
    MODEL_DIR = "iic/SenseVoiceSmall"

def transcribe_audio(
    audio_file: str, 
    start: float, 
    end: float, 
    language: str = "auto"
) -> Dict:
    """
    è½¬å½•éŸ³é¢‘ç‰‡æ®µ
    
    :param audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    :param start: å¼€å§‹æ—¶é—´
    :param end: ç»“æŸæ—¶é—´
    :param language: è¯­è¨€ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
    :return: è½¬å½•ç»“æœå­—å…¸
    """
    try:
        # é€‰æ‹©è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # åŠ è½½ SenseVoice æ¨¡å‹
        m, kwargs = SenseVoiceSmall.from_pretrained(
            model=MODEL_DIR, 
            device=device
        )
        m.eval()

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_audio:
            temp_audio_path = temp_audio.name

        # ä½¿ç”¨ ffmpeg åˆ‡å‰²éŸ³é¢‘
        ffmpeg_cmd = (
            f'ffmpeg -y -i "{audio_file}" '
            f'-ss {start} -t {end-start} '
            '-vn -b:a 64k -ar 16000 -ac 1 '
            f'-metadata encoding=UTF-8 -f mp3 "{temp_audio_path}"'
        )
        
        subprocess.run(ffmpeg_cmd, shell=True, check=True, capture_output=True)

        # ä½¿ç”¨è¿›åº¦æ¡
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            TimeElapsedColumn(),
            transient=True
        ) as progress:
            task = progress.add_task("[cyan]è½¬å½•ä¸­...", total=None)
            
            # ä½¿ç”¨ SenseVoice è¿›è¡Œè½¬å½•
            res = m.inference(
                data_in=temp_audio_path,
                language=language,
                use_itn=False,
                ban_emo_unk=False,
                **kwargs
            )
            progress.update(task, completed=True)

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_audio_path)

        # å¤„ç†è½¬å½•ç»“æœ
        processed_text = rich_transcription_postprocess(res[0][0]["text"])
        
        # æ„é€ ç»“æœ
        result = {
            'segments': [{
                'start': start,
                'end': end,
                'text': processed_text,
                'words': []  # SenseVoice å¯èƒ½ä¸æä¾›é€è¯æ—¶é—´æˆ³
            }]
        }

        return result

    except Exception as e:
        logger.error(f"éŸ³é¢‘è½¬å½•é”™è¯¯: {e}")
        raise

def transcribe(
    video_file: str, 
    output_dir: str = "output", 
    language: str = "auto"
):
    """
    å¤„ç†æ•´ä¸ªè§†é¢‘çš„è½¬å½•
    
    :param video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
    :param output_dir: è¾“å‡ºç›®å½•
    :param language: è¯­è¨€ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        log_dir = os.path.join(output_dir, "log")
        os.makedirs(log_dir, exist_ok=True)

        output_log = os.path.join(log_dir, "cleaned_chunks.xlsx")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬å½•ç»“æœ
        if os.path.exists(output_log):
            rprint("[yellow]âš ï¸ è½¬å½•ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡è½¬å½•æ­¥éª¤.[/yellow]")
            return
        
        # è§†é¢‘è½¬éŸ³é¢‘
        audio_file = convert_video_to_audio(video_file)

        # Demucs äººå£°åˆ†ç¦»
        vocal_audio_path = os.path.join(AUDIO_DIR, VOCAL_AUDIO_FILE)
        background_audio_path = os.path.join(AUDIO_DIR, BACKGROUND_AUDIO_FILE)
        
        if os.path.exists(background_audio_path):
            rprint(f"[yellow]âš ï¸ {background_audio_path} å·²å­˜åœ¨ï¼Œè·³è¿‡ Demucs å¤„ç†.[/yellow]")
        else:
            demucs_main(
                os.path.join(AUDIO_DIR, RAW_AUDIO_FILE),
                AUDIO_DIR,
                background_audio_path,
                vocal_audio_path
            )
            rprint("Demucs å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜äººå£°å’ŒèƒŒæ™¯éŸ³é¢‘")
        
        # ä½¿ç”¨äººå£°éŸ³é¢‘
        audio_file = vocal_audio_path

        # åˆ†å‰²éŸ³é¢‘
        segments = split_audio(audio_file)
        
        # è½¬å½•éŸ³é¢‘
        all_results = []
        for start, end in segments:
            result = transcribe_audio(
                audio_file, 
                start, 
                end, 
                language=language
            )
            all_results.append(result)
        
        # åˆå¹¶ç»“æœ
        combined_result = {'segments': []}
        for result in all_results:
            combined_result['segments'].extend(result['segments'])
        
        # åˆ›å»º DataFrame
        df = pd.DataFrame([
            {
                'start': segment['start'], 
                'end': segment['end'], 
                'text': segment['text']
            } for segment in combined_result['segments']
        ])
        
        # ä¿å­˜ç»“æœ
        save_results(df, output_dir=output_dir)
        
        rprint("[green]âœ… è½¬å½•å®Œæˆ[/green]")

    except Exception as e:
        logger.error(f"è§†é¢‘è½¬å½•å¤„ç†é”™è¯¯: {e}")
        rprint(f"[red]âŒ è½¬å½•å¤±è´¥: {e}[/red]")
        raise

def find_video_files(directory=None):
    """
    æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    
    :param directory: æœç´¢ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
    :return: æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶è·¯å¾„
    """
    if directory is None:
        directory = os.getcwd()
    
    video_extensions = ['.mp4', '.avi', '.mkv', '.mov']
    
    for root, _, files in os.walk(directory):
        for file in files:
            if any(file.lower().endswith(ext) for ext in video_extensions):
                return os.path.join(root, file)
    
    raise FileNotFoundError("æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")

if __name__ == "__main__":
    try:
        video_file = find_video_files()
        rprint(f"[green]ğŸ“ æ‰¾åˆ°è§†é¢‘æ–‡ä»¶:[/green] {video_file}, [green]å¼€å§‹è½¬å½•...[/green]")
        transcribe(video_file)
    except Exception as e:
        rprint(f"[red]âŒ å¤„ç†å¤±è´¥: {e}[/red]")
